

<details>

<summary><strong>Support HackTricks and get benefits!</strong></summary>

<summary> <strong>æ”¯æŒhacktrickså¹¶è·å¾—å¥½å¤„ï¼</strong> </summary>

- Do you work in a **cybersecurity company**? Do you want to see your **company advertised in HackTricks**? or do you want to have access to the **latest version of the PEASS or download HackTricks in PDF**? Check the [**SUBSCRIPTION PLANS**](https://github.com/sponsors/carlospolop)!

 - æ‚¨åœ¨**ç½‘ç»œå®‰å…¨å…¬å¸**å·¥ä½œå—ï¼Ÿ æ‚¨æ˜¯å¦æƒ³çœ‹åˆ°æ‚¨çš„**å…¬å¸åœ¨hacktricks **ä¸­åˆŠç™»å¹¿å‘Šï¼Ÿ è¿˜æ˜¯æ‚¨æƒ³è®¿é—®**æœ€æ–°ç‰ˆæœ¬çš„è±Œè±†æˆ–åœ¨pdf **ä¸­ä¸‹è½½hacktricksï¼Ÿ æ£€æŸ¥[**è®¢é˜…è®¡åˆ’**]ï¼ˆhttps://github.com/sponsors/carlospolopï¼‰ï¼

- Discover [**The PEASS Family**](https://opensea.io/collection/the-peass-family), our collection of exclusive [**NFTs**](https://opensea.io/collection/the-peass-family)

 - å‘ç°[**è±Œè±†å®¶åº­**]ï¼ˆhttps://opensea.io/collection/the-peass-familyï¼‰ï¼Œæˆ‘ä»¬çš„ç‹¬å®¶[** nfts **]ï¼ˆhttps://opensea.io/collectionï¼‰ /å®¶åº­å®¶åº­ï¼‰

- Get the [**official PEASS & HackTricks swag**](https://peass.creator-spring.com)

 - è·å–[**å®˜æ–¹è±Œè±†å’Œhacktricksèµƒç‰©**]ï¼ˆhttps://peass.creator-spring.comï¼‰

- **Join the** [**ğŸ’¬**](https://emojipedia.org/speech-balloon/) [**Discord group**](https://discord.gg/hRep4RUj7f) or the [**telegram group**](https://t.me/peass) or **follow** me on **Twitter** [**ğŸ¦**](https://github.com/carlospolop/hacktricks/tree/7af18b62b3bdc423e11444677a6a73d4043511e9/\[https:/emojipedia.org/bird/README.md)[**@carlospolopm**](https://twitter.com/carlospolopm)**.**

 -  **åŠ å…¥** [**ğŸ’¬**]ï¼ˆhttps://emojipedia.org/speech-balloon/ï¼‰[** discord group **]ï¼ˆhttps://discord.gg/hrep4ruj7fï¼‰æˆ–[ **ç”µæŠ¥ç»„**]ï¼ˆhttps://t.me/peassï¼‰æˆ–**åœ¨** Twitter ** [**ğŸ¦**]ï¼ˆhttps://github.com/carloppolop/hacktrickss on ** twitter **ï¼‰ /ree/7af18b62b3bdc423e114444444677a6a73d4043511e9/ \ [https:/emojipedia.org/bird/bird/readme.mdï¼‰eardme.mdï¼‰eghterme.mdï¼‰eghterme.mdï¼‰eghterme.mdï¼‰eghtemplopmbyth

- **Share your hacking tricks by submitting PRs to the** [**hacktricks github repo**](https://github.com/carlospolop/hacktricks)**.**

 -  **é€šè¿‡å°†PRSæäº¤ç»™** [** hacktricks github repo **]ï¼ˆhttps://github.com/carloppolop/hacktricksï¼‰**ã€‚

</details>


# Basic types of possible data

ï¼ƒå¯èƒ½çš„æ•°æ®çš„åŸºæœ¬ç±»å‹

Data can be **continuous** (**infinity** values) or **categorical** (nominal) where the amount of possible values are **limited**.

æ•°æ®å¯ä»¥æ˜¯**è¿ç»­**ï¼ˆ** infinity **å€¼ï¼‰æˆ–**åˆ†ç±»**ï¼ˆåä¹‰ï¼‰ï¼Œå…¶ä¸­å¯èƒ½å€¼çš„æ•°é‡**æœ‰é™**ã€‚

## Categorical types

### Binary

Just **2 possible values**: 1 or 0. In case in a dataset the values are in string format (e.g. "True" and "False") you assign numbers to those values with:

åªæ˜¯** 2å¯èƒ½çš„å€¼**ï¼š1æˆ–0ã€‚åœ¨æ•°æ®é›†ä¸­ï¼Œå€¼ä»¥å­—ç¬¦ä¸²æ ¼å¼ï¼ˆä¾‹å¦‚â€œ trueâ€å’Œâ€œ falseâ€ï¼‰ï¼Œæ‚¨å°†æ•°å­—åˆ†é…ç»™è¿™äº›å€¼ï¼š

```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```

### **Ordinal**

The **values follows an order**, like in: 1st place, 2nd place... If the categories are strings (like: "starter", "amateur", "professional", "expert") you can map them to numbers as we saw in the binary case.

**å€¼éµå¾ªè®¢å•**ï¼Œä¾‹å¦‚ï¼šç¬¬ä¸€åï¼Œç¬¬äºŒå...å¦‚æœç±»åˆ«æ˜¯å­—ç¬¦ä¸²ï¼ˆä¾‹å¦‚ï¼šâ€œå¯åŠ¨å™¨â€ï¼Œâ€œä¸šä½™â€ï¼Œâ€œä¸“ä¸šâ€ï¼Œâ€œä¸“å®¶â€ï¼‰ï¼Œæ‚¨å¯ä»¥å°†å®ƒä»¬æ˜ å°„åˆ° æ­£å¦‚æˆ‘ä»¬åœ¨äºŒè¿›åˆ¶æ¡ˆä¾‹ä¸­çœ‹åˆ°çš„æ•°å­—ã€‚

```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```

* For **alphabetic columns** you can order them more easily:

*å¯¹äº**å­—æ¯åˆ—**æ‚¨å¯ä»¥æ›´è½»æ¾åœ°è®¢è´­å®ƒä»¬ï¼š

```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```

### **Cyclical**

Looks **like ordinal value** because there is an order, but it doesn't mean one is bigger than the other. Also the **distance between them depends on the direction** you are counting. Example: The days of the week, Sunday isn't "bigger" than Monday.

çœ‹èµ·æ¥**å–œæ¬¢åºæ•°å€¼**ï¼Œå› ä¸ºæœ‰è®¢å•ï¼Œä½†è¿™å¹¶ä¸æ„å‘³ç€ä¸€ä¸ªæ¯”å¦ä¸€ä¸ªå¤§ã€‚ åŒæ ·ï¼Œå®ƒä»¬ä¹‹é—´çš„è·ç¦»å–å†³äºæ‚¨è¦è®¡ç®—çš„æ–¹å‘**ã€‚ ç¤ºä¾‹ï¼šä¸€å‘¨çš„æ—¥å­ï¼Œå‘¨æ—¥ä¸æ¯”æ˜ŸæœŸä¸€â€œå¤§â€ã€‚

* There are **different ways** to encode cyclical features, ones may work with only just some algorithms. **In general, dummy encode can be used**

***æœ‰ä¸åŒçš„æ–¹æ³•**ç¼–ç å‘¨æœŸæ€§ç‰¹å¾ï¼Œå…¶ä¸­ä¸€ä¸ªæ–¹æ³•å¯èƒ½ä»…å¤„ç†ä¸€äº›ç®—æ³•ã€‚ **é€šå¸¸ï¼Œå¯ä»¥ä½¿ç”¨è™šæ‹Ÿç¼–ç **

```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```

### **Dates**

Date are **continuous** **variables**. Can be seen as **cyclical** (because they repeat) **or** as **ordinal** variables (because a time is bigger than a previous one).

æ—¥æœŸæ˜¯**è¿ç»­** **å˜é‡**ã€‚ å¯ä»¥çœ‹ä½œæ˜¯**å‘¨æœŸæ€§**ï¼ˆå› ä¸ºå®ƒä»¬é‡å¤ï¼‰**æˆ–**ä¸º** ordinal **å˜é‡ï¼ˆå› ä¸ºæ—¶é—´æ¯”ä»¥å‰çš„æ—¶é—´å¤§ï¼‰ã€‚

* Usually dates are used as **index**

*é€šå¸¸å°†æ—¥æœŸç”¨ä½œ**ç´¢å¼•**

```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```

### Multi-category/nominal

**More than 2 categories** with no related order. Use `dataset.describe(include='all')` to get information about the categories of each feature.

**è¶…è¿‡2ä¸ªç±»åˆ«**æ²¡æœ‰ç›¸å…³é¡ºåºã€‚ ä½¿ç”¨`dataSet.describeï¼ˆinclude ='all'ï¼‰`ä»¥è·å–æœ‰å…³æ¯ä¸ªåŠŸèƒ½ç±»åˆ«çš„ä¿¡æ¯ã€‚

* A **referring string** is a **column that identifies an example** (like a name of a person). This can be duplicated (because 2 people may have the same name) but most will be unique. This data is **useless and should be removed**.

*a **å¼•ç”¨å­—ç¬¦ä¸²**æ˜¯ä¸€ä¸ª**åˆ—ï¼Œæ ‡è¯†ä¸€ä¸ªç¤ºä¾‹**ï¼ˆå¦‚ä¸€ä¸ªäººçš„åå­—ï¼‰ã€‚ è¿™å¯ä»¥é‡å¤ï¼ˆå› ä¸º2ä¸ªäººå¯èƒ½å…·æœ‰ç›¸åŒçš„åå­—ï¼‰ï¼Œä½†å¤§å¤šæ•°éƒ½æ˜¯å”¯ä¸€çš„ã€‚ è¿™äº›æ•°æ®**æ— ç”¨ï¼Œåº”åˆ é™¤**ã€‚
* A **key column** is used to **link data between tables**. In this case the elements are unique. his data is **useless and should be removed**.

*A **é”®åˆ—**ç”¨äº**é“¾æ¥æ•°æ®ä¹‹é—´çš„æ•°æ®**ã€‚ åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå…ƒç´ æ˜¯å”¯ä¸€çš„ã€‚ ä»–çš„æ•°æ®**æ¯«æ— ç”¨å¤„ï¼Œåº”åˆ é™¤**ã€‚

To **encode multi-category columns into numbers** (so the ML algorithm understand them), **dummy encoding is used** (and **not one-hot encoding** because it **doesn't avoid perfect multicollinearity**).

è¦**ç¼–ç å¤šç±»åˆ—** **ï¼ˆå› æ­¤MLç®—æ³•äº†è§£å®ƒä»¬ï¼‰ï¼Œ**ä½¿ç”¨è™šæ‹Ÿç¼–ç **ï¼ˆå¹¶ä¸”**ä¸æ˜¯ä¸€ä¸ªhotç¼–ç ** **ï¼‰ã€‚

You can get a **multi-category column one-hot encoded** with `pd.get_dummies(dataset.column1)`. This will transform all the classes in binary features, so this will create **one new column per possible class** and will assign 1 **True value to one column**, and the rest will be false.

æ‚¨å¯ä»¥ä½¿ç”¨`pd.get_dummiesï¼ˆdataset.column1ï¼‰````````'' è¿™å°†åœ¨äºŒè¿›åˆ¶åŠŸèƒ½ä¸­è½¬æ¢æ‰€æœ‰ç±»ï¼Œå› æ­¤ï¼Œè¿™å°†åˆ›å»º**æ¯ä¸ªå¯èƒ½çš„ç±»**ä¸€ä¸ªæ–°åˆ—**ï¼Œå¹¶å°†1 ** trueå€¼åˆ†é…ç»™ä¸€åˆ—**ï¼Œå…¶ä½™çš„å°†æ˜¯é”™è¯¯çš„ã€‚

You can get a **multi-category column dummie encoded** with `pd.get_dummies(dataset.column1, drop_first=True)`. This will transform all the classes in binary features, so this will create **one new column per possible class minus one** as the **last 2 columns will be reflect as "1" or "0" in the last binary column created**. This will avoid perfect multicollinearity, reducing the relations between columns.

æ‚¨å¯ä»¥ä½¿ç”¨`pd.get_dummiesï¼ˆdataset.column1ï¼Œdrop_first = trueï¼‰```````'' è¿™å°†åœ¨äºŒè¿›åˆ¶åŠŸèƒ½ä¸­è½¬æ¢æ‰€æœ‰ç±»ï¼Œå› æ­¤ï¼Œè¿™å°†åˆ›å»º**æ¯ä¸ªå¯èƒ½çš„class minus One **ä¸€ä¸ªæ–°åˆ—ï¼Œå› ä¸º**ä¸Šä¸€ä¸ª2åˆ—å°†åœ¨åˆ›å»ºçš„æœ€åä¸€ä¸ªäºŒè¿›åˆ¶åˆ—ä¸­åæ˜ ä¸ºâ€œ 1â€æˆ–â€œ 0â€ **ã€‚ è¿™å°†é¿å…å®Œç¾çš„å¤šé‡å…±çº¿æ€§ï¼Œä»è€Œå‡å°‘åˆ—ä¹‹é—´çš„å…³ç³»ã€‚

# Collinear/Multicollinearity

ï¼ƒåˆ†çº¿/å¤šé‡å…±çº¿æ€§

Collinear appears when **2 features are related to each other**. Multicollineratity appears when those are more than 2.

å½“** 2åŠŸèƒ½å½¼æ­¤ç›¸å…³æ—¶**æ—¶å‡ºç°ç•Œçº¿ã€‚ å½“å®ƒä»¬è¶…è¿‡2ä¸ªæ—¶ï¼Œä¼šå‡ºç°å¤šç¼©æ°´æ€§ã€‚

In ML **you want that your features are related with the possible results but you don't want them to be related between them**. That's why the **dummy encoding mix the last two columns** of that and **is better than one-hot encoding** which doesn't do that creating a clear relation between all the new featured from the multi-category column.

åœ¨ML **ä¸­ï¼Œæ‚¨å¸Œæœ›æ‚¨çš„åŠŸèƒ½ä¸å¯èƒ½çš„ç»“æœç›¸å…³ï¼Œä½†æ‚¨ä¸å¸Œæœ›å®ƒä»¬ä¹‹é—´çš„åŠŸèƒ½**ã€‚ è¿™å°±æ˜¯ä¸ºä»€ä¹ˆ**è™šæ‹Ÿç¼–ç æ··éŸ³çš„æœ€åä¸¤åˆ—**å’Œ**æ¯”ä¸€ä¸ªå£ç‚‰ç¼–ç **æ›´å¥½ï¼Œè¿™å¹¶ä¸æ˜¯è¿™æ ·åšï¼Œè¿™åœ¨å¤šç±»åˆ«åˆ—ä¸­æ‰€æœ‰æ–°åŠŸèƒ½ä¹‹é—´éƒ½æœ‰æ˜ç¡®çš„å…³ç³»ã€‚

VIF is the **Variance Inflation Factor** which **measures the multicollinearity of the features**. A value **above 5 means that one of the two or more collinear features should be removed**.

VIFæ˜¯**æ–¹å·®é€šèƒ€å› å­**ï¼Œå®ƒå¯ä»¥æµ‹é‡åŠŸèƒ½çš„å¤šé‡å…±çº¿æ€§**ã€‚ 5ä¸Šé«˜äº5çš„å€¼è¡¨ç¤ºåº”åˆ é™¤ä¸¤ä¸ªæˆ–å¤šä¸ªå…±çº¿ç‰¹å¾ä¹‹ä¸€**ã€‚

```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```

# Categorical Imbalance

This occurs when there is **not the same amount of each category** in the training data.

å½“åŸ¹è®­æ•°æ®ä¸­çš„æ¯ä¸ªç±»åˆ«**çš„æ•°é‡ä¸åŒæ—¶ï¼Œå°±ä¼šå‘ç”Ÿè¿™ç§æƒ…å†µã€‚

```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```

In an imbalance there is always a **majority class or classes** and a **minority class or classes**.

åœ¨ä¸å¹³è¡¡çš„æƒ…å†µä¸‹ï¼Œæ€»æ˜¯æœ‰ä¸€ä¸ª**å¤šæ•°æ´¾æˆ–è¯¾ç¨‹**ä»¥åŠ**å°‘æ•°æ—è£”æˆ–ç±»**ã€‚

There are 2 main ways to fix this problem:

è§£å†³æ­¤é—®é¢˜çš„ä¸»è¦æ–¹æ³•æœ‰ä¸¤ç§ï¼š

* **Undersampling**: Removing randomly selected data from the majority class so it has the same number of samples as the minority class.

***åº•é‡‡æ ·**ï¼šä»å¤šæ•°ç±»ä¸­åˆ é™¤éšæœºé€‰æ‹©çš„æ•°æ®ï¼Œå› æ­¤å®ƒå…·æœ‰ä¸å°‘æ•°ç±»åˆ«ç›¸åŒçš„æ ·æœ¬æ•°é‡ã€‚

```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```

* **Oversampling**: Generating more data for the minority class until it has as many samples as the majority class.

***è¿‡é‡‡æ ·**ï¼šä¸ºå°‘æ•°æ—è£”ç±»ç”Ÿæˆæ›´å¤šæ•°æ®ï¼Œç›´åˆ°æ‹¥æœ‰ä¸å¤šæ•°ç±»ä¸€æ ·å¤šçš„æ ·æœ¬ä¸ºæ­¢ã€‚

```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```

You can use the argument **`sampling_strategy`** to indicate the **percentage** you want to **undersample or oversample** (**by default it's 1 (100%)** which means to equal the number of minority classes with majority classes)

æ‚¨å¯ä»¥ä½¿ç”¨å‚æ•°**`sampling_strategy` **æŒ‡ç¤º**ç™¾åˆ†æ¯”**æ‚¨æƒ³**ä¸‹é¢æ ·æœ¬æˆ–è¿‡åº¦æ ·æœ¬**ï¼ˆé»˜è®¤æƒ…å†µä¸‹ä¸º1ï¼ˆ100ï¼…ï¼‰**ï¼Œè¿™æ„å‘³ç€ç­‰äº å…·æœ‰å¤šæ•°ç­çº§çš„å°‘æ•°æ—è£”ï¼‰

{% hint style="info" %}

{ï¼…æç¤ºæ ·å¼=â€œ infoâ€ï¼…}
Undersamplig or Oversampling aren't perfect if you get statistics (with `.describe()`) of the over/under-sampled data and compare them to the original you will see **that they changed.** Therefore oversampling and undersampling are modifying the training data.

å¦‚æœæ‚¨è·å¾—è¿‡åº¦/ä¸é‡‡æ ·æ•°æ®çš„ç»Ÿè®¡æ•°æ®ï¼ˆä½¿ç”¨`.deScribeï¼ˆï¼‰`ï¼‰ï¼Œå¹¶å°†å…¶ä¸åŸå§‹å›¾è¿›è¡Œæ¯”è¾ƒï¼Œé‚£ä¹ˆ**å› æ­¤ï¼Œå®‰é‡‡æ ·å’Œåº•é‡‡æ ·æ˜¯ä¸å®Œç¾çš„ã€‚ ä¿®æ”¹åŸ¹è®­æ•°æ®ã€‚
{% endhint %}

## SMOTE oversampling

**SMOTE** is usually a **more trustable way to oversample the data**.

** smote **é€šå¸¸æ˜¯**è¶…å‡ºæ•°æ®**çš„æ›´å¯ä¿¡èµ–çš„æ–¹æ³•ã€‚

```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```

# Rarely Occurring Categories

ï¼ƒå¾ˆå°‘å‘ç”Ÿç±»åˆ«

Imagine a dataset where one of the target classes **occur very little times**.

æƒ³è±¡ä¸€ä¸ªæ•°æ®é›†ï¼Œå…¶ä¸­ä¸€ä¸ªç›®æ ‡ç±»**å‘ç”Ÿçš„æ—¶é—´å¾ˆå°‘**ã€‚

This is like the category imbalance from the previous section, but the rarely occurring category is occurring even less than "minority class" in that case. The **raw** **oversampling** and **undersampling** methods could be also used here, but generally those techniques **won't give really good results**.

è¿™å°±åƒä¸Šä¸€èŠ‚çš„ç±»åˆ«å¤±è¡¡ï¼Œä½†æ˜¯åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œå‘ç”Ÿçš„ç±»åˆ«å¾ˆå°‘å‘ç”Ÿçš„ç±»åˆ«ç”šè‡³å°äºâ€œå°‘æ•°æ—è£”â€ã€‚ ** raw ** **è¿‡åº¦é‡‡æ ·**å’Œ**åº•æ¼†**æ–¹æ³•ä¹Ÿå¯ä»¥åœ¨è¿™é‡Œä½¿ç”¨ï¼Œä½†æ˜¯é€šå¸¸è¿™äº›æŠ€æœ¯**ä¸ä¼šç»™å‡ºéå¸¸å¥½çš„ç»“æœ**ã€‚

## Weights

In some algorithms it's possible to **modify the weights of the targeted data** so some of them get by default more importance when generating the model.

åœ¨æŸäº›ç®—æ³•ä¸­ï¼Œå¯ä»¥**ä¿®æ”¹ç›®æ ‡æ•°æ®çš„æƒé‡**ï¼Œå› æ­¤ï¼Œåœ¨ç”Ÿæˆæ¨¡å‹æ—¶ï¼Œå…¶ä¸­ä¸€äº›é»˜è®¤æƒ…å†µä¸‹æ›´é‡è¦ã€‚

```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```

You can **mix the weights with over/under-sampling techniques** to try to improve the results.

æ‚¨å¯ä»¥**å°†æƒé‡ä¸è¿‡åº¦/ä¸‹é‡‡æ ·æŠ€æœ¯æ··åˆ**ï¼Œä»¥æ”¹å–„ç»“æœã€‚

## PCA - Principal Component Analysis

## PCA-ä¸»è¦ç»„ä»¶åˆ†æ

Is a method that helps to reduce the dimensionality of the data. It's going to **combine different features** to **reduce the amount** of them generating **more useful features** (_less computation is needed_).

æ˜¯ä¸€ç§æœ‰åŠ©äºé™ä½æ•°æ®ç»´åº¦çš„æ–¹æ³•ã€‚ å®ƒå°†**ç»“åˆä¸åŒçš„åŠŸèƒ½**ï¼Œä»¥å‡å°‘å®ƒä»¬ç”Ÿæˆ**çš„æ•°é‡**æ›´æœ‰ç”¨çš„åŠŸèƒ½**ï¼ˆ_æ— éœ€è®¡ç®—_ï¼‰ã€‚

The resulting features aren't understandable by humans, so it also **anonymize the data**.

äººç±»æ— æ³•ç†è§£æ‰€äº§ç”Ÿçš„åŠŸèƒ½ï¼Œå› æ­¤å®ƒä¹Ÿ**åŒ¿åæ•°æ®**ã€‚

# Incongruent Label Categories

Data might have mistakes for unsuccessful transformations or just because human error when writing the data.

æ•°æ®å¯èƒ½ä¼šå‡ºç°é”™è¯¯è½¬æ¢çš„é”™è¯¯ï¼Œæˆ–è€…ä»…ä»…æ˜¯å› ä¸ºç¼–å†™æ•°æ®æ—¶äººä¸ºé”™è¯¯ã€‚

Therefore you might find the **same label with spelling mistakes**, different **capitalisation**, **abbreviations** like: _BLUE, Blue, b, bule_. You need to fix these label errors inside the data before training the model.

å› æ­¤ï¼Œæ‚¨å¯èƒ½ä¼šå‘ç°**å¸¦æœ‰æ‹¼å†™é”™è¯¯çš„åŒä¸€æ ‡ç­¾**ï¼Œä¸åŒçš„**å¤§å†™**ï¼Œ**ç¼©å†™**å–œæ¬¢ï¼š_ bllueï¼Œblueï¼Œbï¼Œbule_ã€‚ åœ¨è®­ç»ƒæ¨¡å‹ä¹‹å‰ï¼Œæ‚¨éœ€è¦åœ¨æ•°æ®ä¸­ä¿®å¤è¿™äº›æ ‡ç­¾é”™è¯¯ã€‚

You can clean this issues by lowercasing everything and mapping misspelled labels to the correct ones.

æ‚¨å¯ä»¥é€šè¿‡é™ä½æ‰€æœ‰å†…å®¹å¹¶å°†æ‹¼å†™é”™è¯¯çš„æ ‡ç­¾æ˜ å°„åˆ°æ­£ç¡®çš„æ ‡ç­¾æ¥æ¸…ç†æ­¤é—®é¢˜ã€‚

It's very important to check that **all the data that you have contains is correctly labeled**, because for example, one misspelling error in the data, when dummie encoding the classes, will generate a new column in the final features with **bad consequences for the final model**. This example can be detected very easily by one-hot encoding a column and checking the names of the columns created.

éå¸¸é‡è¦çš„æ˜¯ï¼Œæ£€æŸ¥**æ‚¨æ‰€åŒ…å«çš„æ‰€æœ‰æ•°æ®æ˜¯å¦æ­£ç¡®æ ‡è®°**ï¼Œå› ä¸ºä¾‹å¦‚ï¼Œå½“æ•°æ®ç¼–ç ç±»æ—¶ï¼Œæ•°æ®ä¸­çš„ä¸€ä¸ªæ‹¼å†™é”™è¯¯å°†åœ¨æœ€ç»ˆåŠŸèƒ½ä¸­ç”Ÿæˆä¸€ä¸ªæ–°åˆ—ï¼Œå¹¶ä½¿ç”¨** æœ€ç»ˆæ¨¡å‹çš„ä¸è‰¯åæœ**ã€‚ å¯ä»¥é€šè¿‡å•çƒ­ç¼–ç åˆ—å¹¶æ£€æŸ¥åˆ›å»ºçš„åˆ—çš„åç§°ï¼Œå¾ˆå®¹æ˜“åœ°æ£€æµ‹åˆ°æ­¤ç¤ºä¾‹ã€‚

# Missing Data

Some data of the study may be missing.

è¯¥ç ”ç©¶çš„ä¸€äº›æ•°æ®å¯èƒ½ç¼ºå°‘ã€‚

It might happen that some complete random data is missing for some error. This is kind of da ta is **Missing Completely at Random** (**MCAR**).

å¯èƒ½ä¼šé‡åˆ°ä¸€äº›é”™è¯¯çš„é”™è¯¯æ•°æ®ã€‚ è¿™æ˜¯ä¸€ç§éšæœº**å®Œå…¨ç¼ºå°‘çš„da taï¼ˆ** mcar **ï¼‰ã€‚

It could be that some random data is missing but there is something making some specific details more probable to be missing, for example more frequently man will tell their their age but not women. This is call **Missing at Random** (**MAR**).

å¯èƒ½æ˜¯ç¼ºå°‘ä¸€äº›éšæœºæ•°æ®ï¼Œä½†æ˜¯æœ‰äº›ä¸œè¥¿ä½¿ä¸€äº›ç‰¹å®šç»†èŠ‚æ›´æœ‰å¯èƒ½ä¸¢å¤±ï¼Œä¾‹å¦‚ï¼Œç”·äººæ›´å¸¸è§åœ°å‘Šè¯‰è‡ªå·±çš„å¹´é¾„ï¼Œè€Œä¸æ˜¯å¥³æ€§ã€‚ è¿™æ˜¯éšæœº**ï¼ˆ** mar **ï¼‰çš„å‘¼å«**ã€‚

Finally, there could be data **Missing Not at Random** (**MNAR**). The vale of the data is directly related with the probability of having the data. For example, if you want to measure something embarrassing, the most embarrassing someone is, the less probable he is going to share it.

æœ€åï¼Œå¯èƒ½ä¼šæœ‰æ•°æ®**ä¸¢å¤±ï¼Œè€Œä¸æ˜¯éšæœº**ï¼ˆ** mnar **ï¼‰ã€‚ æ•°æ®çš„è°·ä¸æ‹¥æœ‰æ•°æ®çš„æ¦‚ç‡ç›´æ¥ç›¸å…³ã€‚ ä¾‹å¦‚ï¼Œå¦‚æœæ‚¨æƒ³è¡¡é‡ä»¤äººå°´å°¬çš„äº‹æƒ…ï¼Œé‚£ä¹ˆæœ€å°´å°¬çš„äººå°±æ˜¯ä»–åˆ†äº«çš„å¯èƒ½æ€§è¶Šå°ã€‚

The **two first categories** of missing data can be **ignorable**. But the **third one** requires to consider **only portions of the data** that isn't impacted or to try to **model the missing data somehow**.

**ç¼ºå°‘æ•°æ®çš„ä¸¤ä¸ªç¬¬ä¸€ç±»**å¯ä»¥**å¯å¿½ç•¥**ã€‚ ä½†æ˜¯ï¼Œ**ç¬¬ä¸‰ä¸ª**éœ€è¦è€ƒè™‘**ä»…å½±å“çš„æ•°æ®çš„éƒ¨åˆ†**æˆ–è¯•å›¾ä»¥æŸç§æ–¹å¼**æ¨¡æ‹Ÿç¼ºå°‘çš„æ•°æ®**ã€‚

One way to find about missing data is to use `.info()` function as it will indicate the **number of rows but also the number of values per category**. If some category has less values than number of rows, then there is some data missing:

æŸ¥æ‰¾ç¼ºå°‘æ•°æ®çš„ä¸€ç§æ–¹æ³•æ˜¯ä½¿ç”¨`infoï¼ˆï¼‰`å‡½æ•°ï¼Œå› ä¸ºå®ƒå°†æŒ‡ç¤º**çš„è¡Œæ•°ï¼Œä¹ŸæŒ‡ç¤ºæ¯ä¸ªç±»åˆ«çš„å€¼æ•°é‡**ã€‚ å¦‚æœæŸäº›ç±»åˆ«çš„å€¼å°‘äºè¡Œæ•°ï¼Œåˆ™ç¼ºå°‘ä¸€äº›æ•°æ®ï¼š

```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```

It's usually recommended that if a feature is **missing in more than the 20%** of the dataset, the **column should be removed:**

é€šå¸¸å»ºè®®ï¼Œå¦‚æœåœ¨æ•°æ®é›†çš„20ï¼…**ä¸­ç¼ºå°‘ä¸€ä¸ªåŠŸèƒ½**ï¼Œåˆ™åº”åˆ é™¤**åˆ—ï¼š**

```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```

{% hint style="info" %}

{ï¼…æç¤ºæ ·å¼=â€œ infoâ€ï¼…}
Note that **not all the missing values are missing in the dataset**. It's possible that missing values have been giving the value "Unknown", "n/a", "", -1, 0... You need to check the dataset (using `dataset.column`_`name.value`_`counts(dropna=False)` to check the possible values).

è¯·æ³¨æ„ï¼Œ**æ•°æ®é›†ä¸­çš„æ‰€æœ‰ç¼ºå¤±å€¼éƒ½æ²¡æœ‰ã€‚ ä¸¢å¤±å€¼å¯èƒ½ä¸€ç›´åœ¨ç»™å‡ºâ€œæœªçŸ¥â€ï¼Œâ€œ n/aâ€ï¼Œâ€œâ€ï¼Œ-1ï¼Œ0 ...æ‚¨éœ€è¦æ£€æŸ¥æ•°æ®é›†ï¼ˆä½¿ç”¨dataset.column`_`name.value`_____________ `è®¡æ•°ï¼ˆdropna = falseï¼‰`ä»¥æ£€æŸ¥å¯èƒ½çš„å€¼ï¼‰ã€‚
{% endhint %}

If some data is missing in the dataset (in it's not too much) you need to find the **category of the missing data**. For that you basically need to know if the **missing data is at random or not**, and for that you need to find if the **missing data was correlated with other data** of the dataset.

å¦‚æœæ•°æ®é›†ä¸­ç¼ºå°‘æŸäº›æ•°æ®ï¼ˆåœ¨ä¸å¤šçš„æ•°æ®é›†ä¸­ï¼‰ï¼Œåˆ™éœ€è¦æ‰¾åˆ°ç¼ºå°‘æ•°æ®çš„**ç±»åˆ«**ã€‚ ä¸ºæ­¤ï¼Œæ‚¨åŸºæœ¬ä¸Šéœ€è¦çŸ¥é“**ä¸¢å¤±çš„æ•°æ®æ˜¯å¦æ˜¯éšæœºçš„**ï¼Œä¸ºæ­¤ï¼Œæ‚¨éœ€è¦æ‰¾åˆ°**ç¼ºå°‘çš„æ•°æ®æ˜¯å¦ä¸æ•°æ®é›†çš„å…¶ä»–æ•°æ®ç›¸å…³è”ã€‚

To find if a missing value if correlated with another column, you can create a new column that put 1s and 0s if the data is missing or isn't and then calculate the correlation between them:

è¦æŸ¥æ‰¾å¦‚æœç¼ºå°‘å€¼ä¸å¦ä¸€åˆ—ç›¸å…³ï¼Œåˆ™å¯ä»¥åˆ›å»ºä¸€ä¸ªæ–°çš„åˆ—ï¼Œè¯¥åˆ—åœ¨ç¼ºå°‘æ•°æ®æˆ–ä¸ä¸¢å¤±çš„æƒ…å†µä¸‹ï¼Œç„¶åè®¡ç®—å®ƒä»¬ä¹‹é—´çš„ç›¸å…³æ€§ï¼š

```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```

If you decide to ignore the missing data you still need to do what to do with it: You can **remove the rows** with missing data (the train data for the model will be smaller), you can r**emove the feature** completely, or could **model it**.

å¦‚æœæ‚¨å†³å®šå¿½ç•¥ä¸¢å¤±çš„æ•°æ®ï¼Œæ‚¨ä»ç„¶éœ€è¦åšçš„äº‹æƒ…ï¼šæ‚¨å¯ä»¥**åˆ é™¤ä½¿ç”¨ä¸¢å¤±çš„æ•°æ®çš„è¡Œ**ï¼ˆæ¨¡å‹çš„ç«è½¦æ•°æ®å°†è¾ƒå°ï¼‰ï¼Œæ‚¨å¯ä»¥r ** emove emove åŠŸèƒ½**å®Œå…¨ï¼Œæˆ–è€…å¯ä»¥**å»ºæ¨¡**ã€‚

You should **check the correlation between the missing feature with the target column** to see how important that feature is for the target, if it's really **small** you can **drop it or fill it**.

æ‚¨åº”è¯¥**æ£€æŸ¥ç¼ºå¤±åŠŸèƒ½ä¸ç›®æ ‡åˆ—ä¹‹é—´çš„ç›¸å…³æ€§**ï¼Œä»¥æŸ¥çœ‹è¯¥åŠŸèƒ½å¯¹ç›®æ ‡çš„é‡è¦æ€§ï¼Œå¦‚æœå®ƒç¡®å®**å°**ï¼Œæ‚¨å¯ä»¥**å°†å…¶ä¸¢å¼ƒæˆ–å¡«å……å®ƒ**ã€‚

To fill missing **continuous data** you could use: the **mean**, the **median** or use an **imputation** algorithm. The imputation algorithm can try to use other features to find a value for the missing feature:

è¦å¡«å†™ç¼ºå¤±çš„**è¿ç»­æ•°æ®**æ‚¨å¯ä»¥ä½¿ç”¨ï¼š**çš„å«ä¹‰**ï¼Œ**ä¸­é—´**æˆ–ä½¿ç”¨**æ’è¡¥**ç®—æ³•ã€‚ æ’å›¾ç®—æ³•å¯ä»¥å°è¯•ä½¿ç”¨å…¶ä»–åŠŸèƒ½æ¥æ‰¾åˆ°ç¼ºå¤±åŠŸèƒ½çš„å€¼ï¼š

```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```

To fill categorical data first of all you need to think if there is any reason why the values are missing. If it's by **choice of the users** (they didn't want to give the data) maybe yo can **create a new category** indicating that. If it's because of human error you can **remove the rows** or the **feature** (check the steps mentioned before) or **fill it with the mode, the most used category** (not recommended).

é¦–å…ˆè¦å¡«å†™åˆ†ç±»æ•°æ®ï¼Œæ‚¨éœ€è¦è€ƒè™‘æ˜¯å¦ç¼ºå°‘å€¼çš„ä»»ä½•ç†ç”±ã€‚ å¦‚æœæ˜¯é€šè¿‡**é€‰æ‹©ç”¨æˆ·**ï¼ˆä»–ä»¬ä¸æƒ³æä¾›æ•°æ®ï¼‰ï¼Œä¹Ÿè®¸æ‚¨å¯ä»¥**åˆ›å»ºä¸€ä¸ªæ–°ç±»åˆ«**è¡¨ç¤ºè¿™ä¸€ç‚¹ã€‚ å¦‚æœæ˜¯å› ä¸ºäººä¸ºé”™è¯¯ï¼Œåˆ™å¯ä»¥**åˆ é™¤è¡Œ**æˆ–**åŠŸèƒ½**ï¼ˆæ£€æŸ¥ä¹‹å‰æåˆ°çš„æ­¥éª¤ï¼‰æˆ–**ä½¿ç”¨è¯¥æ¨¡å¼å¡«å……å®ƒï¼Œæœ€å¸¸ç”¨çš„ç±»åˆ«**ï¼ˆä¸å»ºè®®ä½¿ç”¨ï¼‰ã€‚

# Combining Features

ï¼ƒç»“åˆåŠŸèƒ½

If you find **two features** that are **correlated** between them, usually you should **drop** one of them (the one that is less correlated with the target), but you could also try to **combine them and create a new feature**.

å¦‚æœæ‚¨å‘ç°**ä¸¤ä¸ªåŠŸèƒ½**ç›¸å…³** **ï¼Œé€šå¸¸æ‚¨åº”è¯¥**ä¸¢å¼ƒ**å…¶ä¸­ä¸€ä¸ªï¼ˆä¸ç›®æ ‡è¾ƒå°çš„ç›¸å…³æ€§ï¼‰ï¼Œä½†æ‚¨ä¹Ÿå¯ä»¥å°è¯•** ç»„åˆå®ƒä»¬å¹¶åˆ›å»ºä¸€ä¸ªæ–°åŠŸèƒ½**ã€‚

```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```


<details>

<summary><strong>Support HackTricks and get benefits!</strong></summary>

<summary> <strong>æ”¯æŒhacktrickså¹¶è·å¾—å¥½å¤„ï¼</strong> </summary>

- Do you work in a **cybersecurity company**? Do you want to see your **company advertised in HackTricks**? or do you want to have access to the **latest version of the PEASS or download HackTricks in PDF**? Check the [**SUBSCRIPTION PLANS**](https://github.com/sponsors/carlospolop)!

 - æ‚¨åœ¨**ç½‘ç»œå®‰å…¨å…¬å¸**å·¥ä½œå—ï¼Ÿ æ‚¨æ˜¯å¦æƒ³çœ‹åˆ°æ‚¨çš„**å…¬å¸åœ¨hacktricks **ä¸­åˆŠç™»å¹¿å‘Šï¼Ÿ è¿˜æ˜¯æ‚¨æƒ³è®¿é—®**æœ€æ–°ç‰ˆæœ¬çš„è±Œè±†æˆ–åœ¨pdf **ä¸­ä¸‹è½½hacktricksï¼Ÿ æ£€æŸ¥[**è®¢é˜…è®¡åˆ’**]ï¼ˆhttps://github.com/sponsors/carlospolopï¼‰ï¼

- Discover [**The PEASS Family**](https://opensea.io/collection/the-peass-family), our collection of exclusive [**NFTs**](https://opensea.io/collection/the-peass-family)

 - å‘ç°[**è±Œè±†å®¶åº­**]ï¼ˆhttps://opensea.io/collection/the-peass-familyï¼‰ï¼Œæˆ‘ä»¬çš„ç‹¬å®¶[** nfts **]ï¼ˆhttps://opensea.io/collectionï¼‰ /å®¶åº­å®¶åº­ï¼‰

- Get the [**official PEASS & HackTricks swag**](https://peass.creator-spring.com)

 - è·å–[**å®˜æ–¹è±Œè±†å’Œhacktricksèµƒç‰©**]ï¼ˆhttps://peass.creator-spring.comï¼‰

- **Join the** [**ğŸ’¬**](https://emojipedia.org/speech-balloon/) [**Discord group**](https://discord.gg/hRep4RUj7f) or the [**telegram group**](https://t.me/peass) or **follow** me on **Twitter** [**ğŸ¦**](https://github.com/carlospolop/hacktricks/tree/7af18b62b3bdc423e11444677a6a73d4043511e9/\[https:/emojipedia.org/bird/README.md)[**@carlospolopm**](https://twitter.com/carlospolopm)**.**

 -  **åŠ å…¥** [**ğŸ’¬**]ï¼ˆhttps://emojipedia.org/speech-balloon/ï¼‰[** discord group **]ï¼ˆhttps://discord.gg/hrep4ruj7fï¼‰æˆ–[ **ç”µæŠ¥ç»„**]ï¼ˆhttps://t.me/peassï¼‰æˆ–**åœ¨** Twitter ** [**ğŸ¦**]ï¼ˆhttps://github.com/carloppolop/hacktrickss on ** twitter **ï¼‰ /ree/7af18b62b3bdc423e114444444677a6a73d4043511e9/ \ [https:/emojipedia.org/bird/bird/readme.mdï¼‰eardme.mdï¼‰eghterme.mdï¼‰eghterme.mdï¼‰eghterme.mdï¼‰eghtemplopmbyth

- **Share your hacking tricks by submitting PRs to the** [**hacktricks github repo**](https://github.com/carlospolop/hacktricks)**.**

 -  **é€šè¿‡å°†PRSæäº¤ç»™** [** hacktricks github repo **]ï¼ˆhttps://github.com/carloppolop/hacktricksï¼‰**ã€‚

</details>


