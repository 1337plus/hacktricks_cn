

<details>

<summary><strong>Support HackTricks and get benefits!</strong></summary>

<summary> <strong>支持hacktricks并获得好处！</strong> </summary>

- Do you work in a **cybersecurity company**? Do you want to see your **company advertised in HackTricks**? or do you want to have access to the **latest version of the PEASS or download HackTricks in PDF**? Check the [**SUBSCRIPTION PLANS**](https://github.com/sponsors/carlospolop)!

 - 您在**网络安全公司**工作吗？ 您是否想看到您的**公司在hacktricks **中刊登广告？ 还是您想访问**最新版本的豌豆或在pdf **中下载hacktricks？ 检查[**订阅计划**]（https://github.com/sponsors/carlospolop）！

- Discover [**The PEASS Family**](https://opensea.io/collection/the-peass-family), our collection of exclusive [**NFTs**](https://opensea.io/collection/the-peass-family)

 - 发现[**豌豆家庭**]（https://opensea.io/collection/the-peass-family），我们的独家[** nfts **]（https://opensea.io/collection） /家庭家庭）

- Get the [**official PEASS & HackTricks swag**](https://peass.creator-spring.com)

 - 获取[**官方豌豆和hacktricks赃物**]（https://peass.creator-spring.com）

- **Join the** [**💬**](https://emojipedia.org/speech-balloon/) [**Discord group**](https://discord.gg/hRep4RUj7f) or the [**telegram group**](https://t.me/peass) or **follow** me on **Twitter** [**🐦**](https://github.com/carlospolop/hacktricks/tree/7af18b62b3bdc423e11444677a6a73d4043511e9/\[https:/emojipedia.org/bird/README.md)[**@carlospolopm**](https://twitter.com/carlospolopm)**.**

 -  **加入** [**💬**]（https://emojipedia.org/speech-balloon/）[** discord group **]（https://discord.gg/hrep4ruj7f）或[ **电报组**]（https://t.me/peass）或**在** Twitter ** [**🐦**]（https://github.com/carloppolop/hacktrickss on ** twitter **） /ree/7af18b62b3bdc423e114444444677a6a73d4043511e9/ \ [https:/emojipedia.org/bird/bird/readme.md）eardme.md）eghterme.md）eghterme.md）eghterme.md）eghtemplopmbyth

- **Share your hacking tricks by submitting PRs to the** [**hacktricks github repo**](https://github.com/carlospolop/hacktricks)**.**

 -  **通过将PRS提交给** [** hacktricks github repo **]（https://github.com/carloppolop/hacktricks）**。

</details>


# Basic types of possible data

＃可能的数据的基本类型

Data can be **continuous** (**infinity** values) or **categorical** (nominal) where the amount of possible values are **limited**.

数据可以是**连续**（** infinity **值）或**分类**（名义），其中可能值的数量**有限**。

## Categorical types

### Binary

Just **2 possible values**: 1 or 0. In case in a dataset the values are in string format (e.g. "True" and "False") you assign numbers to those values with:

只是** 2可能的值**：1或0。在数据集中，值以字符串格式（例如“ true”和“ false”），您将数字分配给这些值：

```python
dataset["column2"] = dataset.column2.map({"T": 1, "F": 0})
```

### **Ordinal**

The **values follows an order**, like in: 1st place, 2nd place... If the categories are strings (like: "starter", "amateur", "professional", "expert") you can map them to numbers as we saw in the binary case.

**值遵循订单**，例如：第一名，第二名...如果类别是字符串（例如：“启动器”，“业余”，“专业”，“专家”），您可以将它们映射到 正如我们在二进制案例中看到的数字。

```python
column2_mapping = {'starter':0,'amateur':1,'professional':2,'expert':3}
dataset['column2'] = dataset.column2.map(column2_mapping)
```

* For **alphabetic columns** you can order them more easily:

*对于**字母列**您可以更轻松地订购它们：

```python
# First get all the uniq values alphabetically sorted
possible_values_sorted = dataset.column2.sort_values().unique().tolist()
# Assign each one a value
possible_values_mapping = {value:idx for idx,value in enumerate(possible_values_sorted)}
dataset['column2'] = dataset.column2.map(possible_values_mapping)
```

### **Cyclical**

Looks **like ordinal value** because there is an order, but it doesn't mean one is bigger than the other. Also the **distance between them depends on the direction** you are counting. Example: The days of the week, Sunday isn't "bigger" than Monday.

看起来**喜欢序数值**，因为有订单，但这并不意味着一个比另一个大。 同样，它们之间的距离取决于您要计算的方向**。 示例：一周的日子，周日不比星期一“大”。

* There are **different ways** to encode cyclical features, ones may work with only just some algorithms. **In general, dummy encode can be used**

***有不同的方法**编码周期性特征，其中一个方法可能仅处理一些算法。 **通常，可以使用虚拟编码**

```python
column2_dummies = pd.get_dummies(dataset.column2, drop_first=True)
dataset_joined = pd.concat([dataset[['column2']], column2_dummies], axis=1)
```

### **Dates**

Date are **continuous** **variables**. Can be seen as **cyclical** (because they repeat) **or** as **ordinal** variables (because a time is bigger than a previous one).

日期是**连续** **变量**。 可以看作是**周期性**（因为它们重复）**或**为** ordinal **变量（因为时间比以前的时间大）。

* Usually dates are used as **index**

*通常将日期用作**索引**

```python
# Transform dates to datetime
dataset["column_date"] = pd.to_datetime(dataset.column_date)
# Make the date feature the index
dataset.set_index('column_date', inplace=True)
print(dataset.head())

# Sum usage column per day
daily_sum = dataset.groupby(df_daily_usage.index.date).agg({'usage':['sum']})
# Flatten and rename usage column
daily_sum.columns = daily_sum.columns.get_level_values(0)
daily_sum.columns = ['daily_usage']
print(daily_sum.head())

# Fill days with 0 usage
idx = pd.date_range('2020-01-01', '2020-12-31')
daily_sum.index = pd.DatetimeIndex(daily_sum.index)
df_filled = daily_sum.reindex(idx, fill_value=0) # Fill missing values


# Get day of the week, Monday=0, Sunday=6, and week days names
dataset['DoW'] = dataset.transaction_date.dt.dayofweek
# do the same in a different way
dataset['weekday'] = dataset.transaction_date.dt.weekday
# get day names
dataset['day_name'] = dataset.transaction_date.apply(lambda x: x.day_name())
```

### Multi-category/nominal

**More than 2 categories** with no related order. Use `dataset.describe(include='all')` to get information about the categories of each feature.

**超过2个类别**没有相关顺序。 使用`dataSet.describe（include ='all'）`以获取有关每个功能类别的信息。

* A **referring string** is a **column that identifies an example** (like a name of a person). This can be duplicated (because 2 people may have the same name) but most will be unique. This data is **useless and should be removed**.

*a **引用字符串**是一个**列，标识一个示例**（如一个人的名字）。 这可以重复（因为2个人可能具有相同的名字），但大多数都是唯一的。 这些数据**无用，应删除**。
* A **key column** is used to **link data between tables**. In this case the elements are unique. his data is **useless and should be removed**.

*A **键列**用于**链接数据之间的数据**。 在这种情况下，元素是唯一的。 他的数据**毫无用处，应删除**。

To **encode multi-category columns into numbers** (so the ML algorithm understand them), **dummy encoding is used** (and **not one-hot encoding** because it **doesn't avoid perfect multicollinearity**).

要**编码多类列** **（因此ML算法了解它们），**使用虚拟编码**（并且**不是一个hot编码** **）。

You can get a **multi-category column one-hot encoded** with `pd.get_dummies(dataset.column1)`. This will transform all the classes in binary features, so this will create **one new column per possible class** and will assign 1 **True value to one column**, and the rest will be false.

您可以使用`pd.get_dummies（dataset.column1）````````'' 这将在二进制功能中转换所有类，因此，这将创建**每个可能的类**一个新列**，并将1 ** true值分配给一列**，其余的将是错误的。

You can get a **multi-category column dummie encoded** with `pd.get_dummies(dataset.column1, drop_first=True)`. This will transform all the classes in binary features, so this will create **one new column per possible class minus one** as the **last 2 columns will be reflect as "1" or "0" in the last binary column created**. This will avoid perfect multicollinearity, reducing the relations between columns.

您可以使用`pd.get_dummies（dataset.column1，drop_first = true）```````'' 这将在二进制功能中转换所有类，因此，这将创建**每个可能的class minus One **一个新列，因为**上一个2列将在创建的最后一个二进制列中反映为“ 1”或“ 0” **。 这将避免完美的多重共线性，从而减少列之间的关系。

# Collinear/Multicollinearity

＃分线/多重共线性

Collinear appears when **2 features are related to each other**. Multicollineratity appears when those are more than 2.

当** 2功能彼此相关时**时出现界线。 当它们超过2个时，会出现多缩水性。

In ML **you want that your features are related with the possible results but you don't want them to be related between them**. That's why the **dummy encoding mix the last two columns** of that and **is better than one-hot encoding** which doesn't do that creating a clear relation between all the new featured from the multi-category column.

在ML **中，您希望您的功能与可能的结果相关，但您不希望它们之间的功能**。 这就是为什么**虚拟编码混音的最后两列**和**比一个壁炉编码**更好，这并不是这样做，这在多类别列中所有新功能之间都有明确的关系。

VIF is the **Variance Inflation Factor** which **measures the multicollinearity of the features**. A value **above 5 means that one of the two or more collinear features should be removed**.

VIF是**方差通胀因子**，它可以测量功能的多重共线性**。 5上高于5的值表示应删除两个或多个共线特征之一**。

```python
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant

#dummies_encoded = pd.get_dummies(dataset.column1, drop_first=True)
onehot_encoded = pd.get_dummies(dataset.column1)
X = add_constant(onehot_encoded) # Add previously one-hot encoded data
print(pd.Series([variance_inflation_factor(X.values,i) for i in range(X.shape[1])], index=X.columns))
```

# Categorical Imbalance

This occurs when there is **not the same amount of each category** in the training data.

当培训数据中的每个类别**的数量不同时，就会发生这种情况。

```python
# Get statistic of the features
print(dataset.describe(include='all'))
# Get an overview of the features
print(dataset.info())
# Get imbalance information of the target column
print(dataset.target_column.value_counts())
```

In an imbalance there is always a **majority class or classes** and a **minority class or classes**.

在不平衡的情况下，总是有一个**多数派或课程**以及**少数族裔或类**。

There are 2 main ways to fix this problem:

解决此问题的主要方法有两种：

* **Undersampling**: Removing randomly selected data from the majority class so it has the same number of samples as the minority class.

***底采样**：从多数类中删除随机选择的数据，因此它具有与少数类别相同的样本数量。

```python
from imblearn.under_sampling import RandomUnderSampler
rus = RandomUserSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_under, y_under = rus.fit_resample(X,y)
print(y_under.value_counts()) #Confirm data isn't imbalanced anymore
```

* **Oversampling**: Generating more data for the minority class until it has as many samples as the majority class.

***过采样**：为少数族裔类生成更多数据，直到拥有与多数类一样多的样本为止。

```python
from imblearn.under_sampling import RandomOverSampler
ros = RandomOverSampler(random_state=1337)

X = dataset[['column1', 'column2', 'column3']].copy()
y = dataset.target_column

X_over, y_over = ros.fit_resample(X,y)
print(y_over.value_counts()) #Confirm data isn't imbalanced anymore
```

You can use the argument **`sampling_strategy`** to indicate the **percentage** you want to **undersample or oversample** (**by default it's 1 (100%)** which means to equal the number of minority classes with majority classes)

您可以使用参数**`sampling_strategy` **指示**百分比**您想**下面样本或过度样本**（默认情况下为1（100％）**，这意味着等于 具有多数班级的少数族裔）

{% hint style="info" %}

{％提示样式=“ info”％}
Undersamplig or Oversampling aren't perfect if you get statistics (with `.describe()`) of the over/under-sampled data and compare them to the original you will see **that they changed.** Therefore oversampling and undersampling are modifying the training data.

如果您获得过度/不采样数据的统计数据（使用`.deScribe（）`），并将其与原始图进行比较，那么**因此，安采样和底采样是不完美的。 修改培训数据。
{% endhint %}

## SMOTE oversampling

**SMOTE** is usually a **more trustable way to oversample the data**.

** smote **通常是**超出数据**的更可信赖的方法。

```python
from imblearn.over_sampling import SMOTE

# Form SMOTE the target_column need to be numeric, map it if necessary
smote = SMOTE(random_state=1337)
X_smote, y_smote = smote.fit_resample(dataset[['column1', 'column2', 'column3']], dataset.target_column)
dataset_smote = pd.DataFrame(X_smote, columns=['column1', 'column2', 'column3'])
dataset['target_column'] = y_smote
print(y_smote.value_counts()) #Confirm data isn't imbalanced anymore
```

# Rarely Occurring Categories

＃很少发生类别

Imagine a dataset where one of the target classes **occur very little times**.

想象一个数据集，其中一个目标类**发生的时间很少**。

This is like the category imbalance from the previous section, but the rarely occurring category is occurring even less than "minority class" in that case. The **raw** **oversampling** and **undersampling** methods could be also used here, but generally those techniques **won't give really good results**.

这就像上一节的类别失衡，但是在这种情况下，发生的类别很少发生的类别甚至小于“少数族裔”。 ** raw ** **过度采样**和**底漆**方法也可以在这里使用，但是通常这些技术**不会给出非常好的结果**。

## Weights

In some algorithms it's possible to **modify the weights of the targeted data** so some of them get by default more importance when generating the model.

在某些算法中，可以**修改目标数据的权重**，因此，在生成模型时，其中一些默认情况下更重要。

```python
weights = {0: 10 1:1} #Assign weight 10 to False and 1 to True
model = LogisticRegression(class_weight=weights)
```

You can **mix the weights with over/under-sampling techniques** to try to improve the results.

您可以**将权重与过度/下采样技术混合**，以改善结果。

## PCA - Principal Component Analysis

## PCA-主要组件分析

Is a method that helps to reduce the dimensionality of the data. It's going to **combine different features** to **reduce the amount** of them generating **more useful features** (_less computation is needed_).

是一种有助于降低数据维度的方法。 它将**结合不同的功能**，以减少它们生成**的数量**更有用的功能**（_无需计算_）。

The resulting features aren't understandable by humans, so it also **anonymize the data**.

人类无法理解所产生的功能，因此它也**匿名数据**。

# Incongruent Label Categories

Data might have mistakes for unsuccessful transformations or just because human error when writing the data.

数据可能会出现错误转换的错误，或者仅仅是因为编写数据时人为错误。

Therefore you might find the **same label with spelling mistakes**, different **capitalisation**, **abbreviations** like: _BLUE, Blue, b, bule_. You need to fix these label errors inside the data before training the model.

因此，您可能会发现**带有拼写错误的同一标签**，不同的**大写**，**缩写**喜欢：_ bllue，blue，b，bule_。 在训练模型之前，您需要在数据中修复这些标签错误。

You can clean this issues by lowercasing everything and mapping misspelled labels to the correct ones.

您可以通过降低所有内容并将拼写错误的标签映射到正确的标签来清理此问题。

It's very important to check that **all the data that you have contains is correctly labeled**, because for example, one misspelling error in the data, when dummie encoding the classes, will generate a new column in the final features with **bad consequences for the final model**. This example can be detected very easily by one-hot encoding a column and checking the names of the columns created.

非常重要的是，检查**您所包含的所有数据是否正确标记**，因为例如，当数据编码类时，数据中的一个拼写错误将在最终功能中生成一个新列，并使用** 最终模型的不良后果**。 可以通过单热编码列并检查创建的列的名称，很容易地检测到此示例。

# Missing Data

Some data of the study may be missing.

该研究的一些数据可能缺少。

It might happen that some complete random data is missing for some error. This is kind of da ta is **Missing Completely at Random** (**MCAR**).

可能会遇到一些错误的错误数据。 这是一种随机**完全缺少的da ta（** mcar **）。

It could be that some random data is missing but there is something making some specific details more probable to be missing, for example more frequently man will tell their their age but not women. This is call **Missing at Random** (**MAR**).

可能是缺少一些随机数据，但是有些东西使一些特定细节更有可能丢失，例如，男人更常见地告诉自己的年龄，而不是女性。 这是随机**（** mar **）的呼叫**。

Finally, there could be data **Missing Not at Random** (**MNAR**). The vale of the data is directly related with the probability of having the data. For example, if you want to measure something embarrassing, the most embarrassing someone is, the less probable he is going to share it.

最后，可能会有数据**丢失，而不是随机**（** mnar **）。 数据的谷与拥有数据的概率直接相关。 例如，如果您想衡量令人尴尬的事情，那么最尴尬的人就是他分享的可能性越小。

The **two first categories** of missing data can be **ignorable**. But the **third one** requires to consider **only portions of the data** that isn't impacted or to try to **model the missing data somehow**.

**缺少数据的两个第一类**可以**可忽略**。 但是，**第三个**需要考虑**仅影响的数据的部分**或试图以某种方式**模拟缺少的数据**。

One way to find about missing data is to use `.info()` function as it will indicate the **number of rows but also the number of values per category**. If some category has less values than number of rows, then there is some data missing:

查找缺少数据的一种方法是使用`info（）`函数，因为它将指示**的行数，也指示每个类别的值数量**。 如果某些类别的值少于行数，则缺少一些数据：

```bash
# Get info of the dataset
dataset.info()

# Drop all rows where some value is missing
dataset.dropna(how='any', axis=0).info()
```

It's usually recommended that if a feature is **missing in more than the 20%** of the dataset, the **column should be removed:**

通常建议，如果在数据集的20％**中缺少一个功能**，则应删除**列：**

```bash
# Remove column
dataset.drop('Column_name', axis='columns', inplace=True)
dataset.info()
```

{% hint style="info" %}

{％提示样式=“ info”％}
Note that **not all the missing values are missing in the dataset**. It's possible that missing values have been giving the value "Unknown", "n/a", "", -1, 0... You need to check the dataset (using `dataset.column`_`name.value`_`counts(dropna=False)` to check the possible values).

请注意，**数据集中的所有缺失值都没有。 丢失值可能一直在给出“未知”，“ n/a”，“”，-1，0 ...您需要检查数据集（使用dataset.column`_`name.value`_____________ `计数（dropna = false）`以检查可能的值）。
{% endhint %}

If some data is missing in the dataset (in it's not too much) you need to find the **category of the missing data**. For that you basically need to know if the **missing data is at random or not**, and for that you need to find if the **missing data was correlated with other data** of the dataset.

如果数据集中缺少某些数据（在不多的数据集中），则需要找到缺少数据的**类别**。 为此，您基本上需要知道**丢失的数据是否是随机的**，为此，您需要找到**缺少的数据是否与数据集的其他数据相关联。

To find if a missing value if correlated with another column, you can create a new column that put 1s and 0s if the data is missing or isn't and then calculate the correlation between them:

要查找如果缺少值与另一列相关，则可以创建一个新的列，该列在缺少数据或不丢失的情况下，然后计算它们之间的相关性：

```bash
# The closer it's to 1 or -1 the more correlated the data is
# Note that columns are always perfectly correlated with themselves.
dataset[['column_name', 'cloumn_missing_data']].corr()
```

If you decide to ignore the missing data you still need to do what to do with it: You can **remove the rows** with missing data (the train data for the model will be smaller), you can r**emove the feature** completely, or could **model it**.

如果您决定忽略丢失的数据，您仍然需要做的事情：您可以**删除使用丢失的数据的行**（模型的火车数据将较小），您可以r ** emove emove 功能**完全，或者可以**建模**。

You should **check the correlation between the missing feature with the target column** to see how important that feature is for the target, if it's really **small** you can **drop it or fill it**.

您应该**检查缺失功能与目标列之间的相关性**，以查看该功能对目标的重要性，如果它确实**小**，您可以**将其丢弃或填充它**。

To fill missing **continuous data** you could use: the **mean**, the **median** or use an **imputation** algorithm. The imputation algorithm can try to use other features to find a value for the missing feature:

要填写缺失的**连续数据**您可以使用：**的含义**，**中间**或使用**插补**算法。 插图算法可以尝试使用其他功能来找到缺失功能的值：

```python
from sklearn.impute import KNNImputer

X = dataset[['column1', 'column2', 'column3']]
y = dataset.column_target

# Create the imputer that will fill the data
imputer = KNNImputer(n_neightbors=2, weights='uniform')
X_imp = imputer.fit_transform(X)

# Check new data
dataset_imp = pd.DataFrame(X_imp)
dataset.columns = ['column1', 'column2', 'column3']
dataset.iloc[10:20] # Get some indexes that contained empty data before
```

To fill categorical data first of all you need to think if there is any reason why the values are missing. If it's by **choice of the users** (they didn't want to give the data) maybe yo can **create a new category** indicating that. If it's because of human error you can **remove the rows** or the **feature** (check the steps mentioned before) or **fill it with the mode, the most used category** (not recommended).

首先要填写分类数据，您需要考虑是否缺少值的任何理由。 如果是通过**选择用户**（他们不想提供数据），也许您可以**创建一个新类别**表示这一点。 如果是因为人为错误，则可以**删除行**或**功能**（检查之前提到的步骤）或**使用该模式填充它，最常用的类别**（不建议使用）。

# Combining Features

＃结合功能

If you find **two features** that are **correlated** between them, usually you should **drop** one of them (the one that is less correlated with the target), but you could also try to **combine them and create a new feature**.

如果您发现**两个功能**相关** **，通常您应该**丢弃**其中一个（与目标较小的相关性），但您也可以尝试** 组合它们并创建一个新功能**。

```python
# Create a new feautr combining feature1 and feature2
dataset['new_feature'] = dataset.column1/dataset.column2

# Check correlation with target column
dataset[['new_feature', 'column1', 'column2', 'target']].corr()['target'][:]

# Check for collinearity of the 2 features and the new one
X = add_constant(dataset[['column1', 'column2', 'target']])
# Calculate VIF
pd.Series([variance_inflation_factor(X.values, i) for i in range(X.shape[1])], index=X.columns)
```


<details>

<summary><strong>Support HackTricks and get benefits!</strong></summary>

<summary> <strong>支持hacktricks并获得好处！</strong> </summary>

- Do you work in a **cybersecurity company**? Do you want to see your **company advertised in HackTricks**? or do you want to have access to the **latest version of the PEASS or download HackTricks in PDF**? Check the [**SUBSCRIPTION PLANS**](https://github.com/sponsors/carlospolop)!

 - 您在**网络安全公司**工作吗？ 您是否想看到您的**公司在hacktricks **中刊登广告？ 还是您想访问**最新版本的豌豆或在pdf **中下载hacktricks？ 检查[**订阅计划**]（https://github.com/sponsors/carlospolop）！

- Discover [**The PEASS Family**](https://opensea.io/collection/the-peass-family), our collection of exclusive [**NFTs**](https://opensea.io/collection/the-peass-family)

 - 发现[**豌豆家庭**]（https://opensea.io/collection/the-peass-family），我们的独家[** nfts **]（https://opensea.io/collection） /家庭家庭）

- Get the [**official PEASS & HackTricks swag**](https://peass.creator-spring.com)

 - 获取[**官方豌豆和hacktricks赃物**]（https://peass.creator-spring.com）

- **Join the** [**💬**](https://emojipedia.org/speech-balloon/) [**Discord group**](https://discord.gg/hRep4RUj7f) or the [**telegram group**](https://t.me/peass) or **follow** me on **Twitter** [**🐦**](https://github.com/carlospolop/hacktricks/tree/7af18b62b3bdc423e11444677a6a73d4043511e9/\[https:/emojipedia.org/bird/README.md)[**@carlospolopm**](https://twitter.com/carlospolopm)**.**

 -  **加入** [**💬**]（https://emojipedia.org/speech-balloon/）[** discord group **]（https://discord.gg/hrep4ruj7f）或[ **电报组**]（https://t.me/peass）或**在** Twitter ** [**🐦**]（https://github.com/carloppolop/hacktrickss on ** twitter **） /ree/7af18b62b3bdc423e114444444677a6a73d4043511e9/ \ [https:/emojipedia.org/bird/bird/readme.md）eardme.md）eghterme.md）eghterme.md）eghterme.md）eghtemplopmbyth

- **Share your hacking tricks by submitting PRs to the** [**hacktricks github repo**](https://github.com/carlospolop/hacktricks)**.**

 -  **通过将PRS提交给** [** hacktricks github repo **]（https://github.com/carloppolop/hacktricks）**。

</details>


